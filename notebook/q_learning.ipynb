{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d0589c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states:  500\n",
      "Number of actions:  6\n",
      "Reset state: (133, {'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gym \n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "n_state = env.observation_space.n \n",
    "print(\"Number of states: \", n_state)\n",
    "n_action = env.action_space.n \n",
    "print(\"Number of actions: \", n_action)\n",
    "print(\"Reset state:\", env.reset())\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93cd118",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m gamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     50\u001b[0m alpha \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m---> 51\u001b[0m optimal_Q, optimal_policy \u001b[38;5;241m=\u001b[39m \u001b[43mq_learning\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 31\u001b[0m, in \u001b[0;36mq_learning\u001b[1;34m(env, gamma, n_episodes, alpha)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_done:\n\u001b[0;32m     30\u001b[0m     hashed_state \u001b[38;5;241m=\u001b[39m make_hashable(state)\n\u001b[1;32m---> 31\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mepsilon_greedy_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhashed_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# <-- pass hashed_state here\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     next_state, reward, is_done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     33\u001b[0m     hashed_next_state \u001b[38;5;241m=\u001b[39m make_hashable(next_state)\n",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m, in \u001b[0;36mgen_epsilon_greedy_policy.<locals>.policy_fn\u001b[1;34m(state, Q)\u001b[0m\n\u001b[0;32m     12\u001b[0m probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(n_action)\u001b[38;5;241m*\u001b[39mepsilon \u001b[38;5;241m/\u001b[39m n_action\n\u001b[0;32m     13\u001b[0m hashed_state \u001b[38;5;241m=\u001b[39m make_hashable(state)\n\u001b[1;32m---> 14\u001b[0m best_action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(\u001b[43mQ\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhashed_state\u001b[49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     15\u001b[0m probs[best_action] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m epsilon)\n\u001b[0;32m     16\u001b[0m action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "def make_hashable(obj):\n",
    "    \"\"\"Recursively convert obj to a hashable type.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return tuple(sorted((k, make_hashable(v)) for k, v in obj.items()))\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return tuple(make_hashable(x) for x in obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def gen_epsilon_greedy_policy(n_action, epsilon):\n",
    "    def policy_fn(state, Q):\n",
    "        probs = torch.ones(n_action)*epsilon / n_action\n",
    "        hashed_state = make_hashable(state)\n",
    "        best_action = torch.argmax(Q[hashed_state]).item()\n",
    "        probs[best_action] += (1.0 - epsilon)\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        return action\n",
    "    return policy_fn\n",
    "\n",
    "epsilon = 0.1\n",
    "epsilon_greedy_policy = gen_epsilon_greedy_policy(n_action, epsilon)\n",
    "   \n",
    "def q_learning(env, gamma, n_episodes, alpha):\n",
    "    n_action  = env.action_space.n\n",
    "    Q = defaultdict(lambda: torch.zeros(n_action))\n",
    "    for _ in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        is_done = False\n",
    "        while not is_done:\n",
    "            hashed_state = make_hashable(state)\n",
    "            print(\"this is the hashed state\" , hashed_state)\n",
    "            action = epsilon_greedy_policy(hashed_state, Q)  \n",
    "            next_state, reward, is_done, info = env.step(action)\n",
    "            hashed_next_state = make_hashable(next_state)\n",
    "            print(\"this is the hashed state\" , hashed)\n",
    "            delta = reward + gamma * torch.max(Q[hashed_next_state]) - Q[hashed_state][action]\n",
    "            Q[hashed_state][action] += alpha * delta\n",
    "            length_episode[_] += 1\n",
    "            total_reward_episode[_] += reward\n",
    "            if is_done:\n",
    "                break\n",
    "            state = next_state\n",
    "    policy = {}\n",
    "    for state, action in Q.items():\n",
    "        policy[state] = torch.argmax(action).item()\n",
    "    return Q, policy \n",
    "n_episodes = 1000\n",
    "length_episode = [0]*n_episodes\n",
    "total_reward_episode = [0]*n_episodes\n",
    "\n",
    "gamma = 1\n",
    "alpha =0.1\n",
    "optimal_Q, optimal_policy = q_learning(env, gamma, n_episodes, alpha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
