{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29acd560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available environments:\n",
      "CartPole-v0\n",
      "CartPole-v1\n",
      "MountainCar-v0\n",
      "MountainCarContinuous-v0\n",
      "Pendulum-v1\n",
      "Acrobot-v1\n",
      "LunarLander-v2\n",
      "LunarLanderContinuous-v2\n",
      "BipedalWalker-v3\n",
      "BipedalWalkerHardcore-v3\n",
      "CarRacing-v2\n",
      "Blackjack-v1\n",
      "FrozenLake-v1\n",
      "FrozenLake8x8-v1\n",
      "CliffWalking-v0\n",
      "Taxi-v3\n",
      "Reacher-v2\n",
      "Reacher-v4\n",
      "Pusher-v2\n",
      "Pusher-v4\n",
      "InvertedPendulum-v2\n",
      "InvertedPendulum-v4\n",
      "InvertedDoublePendulum-v2\n",
      "InvertedDoublePendulum-v4\n",
      "HalfCheetah-v2\n",
      "HalfCheetah-v3\n",
      "HalfCheetah-v4\n",
      "Hopper-v2\n",
      "Hopper-v3\n",
      "Hopper-v4\n",
      "Swimmer-v2\n",
      "Swimmer-v3\n",
      "Swimmer-v4\n",
      "Walker2d-v2\n",
      "Walker2d-v3\n",
      "Walker2d-v4\n",
      "Ant-v2\n",
      "Ant-v3\n",
      "Ant-v4\n",
      "Humanoid-v2\n",
      "Humanoid-v3\n",
      "Humanoid-v4\n",
      "HumanoidStandup-v2\n",
      "HumanoidStandup-v4\n",
      "Number of states: 16\n",
      "Number of actions:  4\n"
     ]
    }
   ],
   "source": [
    "import gym \n",
    "import time\n",
    "\n",
    "envs = gym.envs.registry.values()\n",
    "print(\"Available environments:\")\n",
    "for env in envs:\n",
    "    print(env.id)\n",
    "    \n",
    "env = gym.make('FrozenLake-v1', render_mode = 'human', desc=None, map_name=\"4x4\", is_slippery=True)\n",
    "n_state = env.observation_space.n \n",
    "\n",
    "print('Number of states:', n_state)\n",
    "\n",
    "n_action = env.action_space.n \n",
    "\n",
    "print('Number of actions: ', n_action)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "796c2529",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b25f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4352e270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0.0\n",
      "False\n",
      "False\n",
      "{'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "#new_state,info = env.reset()\n",
    "\n",
    "new_state, reward,terminated, truncated, info = env.step(2)\n",
    "print(new_state)\n",
    "print(reward)\n",
    "print(terminated)\n",
    "print(truncated)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f777ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5290d99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running episodes...\n",
      "FrozenLake-v1\n",
      "Number of episodes:  1000\n",
      "Number of states:  16\n",
      "Number of actions:  4\n",
      "Average reward over 1000 episodes: 0.008\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "def run_episode(env, policy):\n",
    "    state,info = env.reset()\n",
    "    total_reward = 0 \n",
    "    steps = 0\n",
    "    is_done = False \n",
    "    while not is_done:\n",
    "        action = policy[state].item()\n",
    "        state, reward, terminated, truncated,info = env.step(action) \n",
    "        is_done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        if is_done:\n",
    "            break\n",
    "    return total_reward\n",
    "    \n",
    "n_episodes = 1000\n",
    "total_rewards = []\n",
    "print(\"Running episodes...\") \n",
    "print(env.spec.id)\n",
    "print(\"Number of episodes: \", n_episodes) \n",
    "print(\"Number of states: \", n_state)\n",
    "print(\"Number of actions: \", n_action) \n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    random_policy = torch.randint( high = n_action, size = (n_state,))\n",
    "    total_reward =  run_episode(env, random_policy)\n",
    "    total_rewards.append(total_reward) \n",
    "\n",
    "print(f\"Average reward over {n_episodes} episodes: {sum(total_rewards)/n_episodes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76225efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition Matrix: {0: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False)], 1: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True)], 2: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False)], 3: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True)]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Transition Matrix:\", env.env.P[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a31c5d24",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 25\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m v \n\u001b[1;32m---> 25\u001b[0m v_optimal \u001b[38;5;241m=\u001b[39m value_iteration(\u001b[43menv\u001b[49m,gamma, threshold)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimal value function: \u001b[39m\u001b[38;5;124m\"\u001b[39m, v_optimal)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "gamma = 0.99\n",
    "threshold = 0.0001\n",
    "\n",
    "#sample of a MDP algorithm understanding the value iteration algorithm\n",
    "def value_iteration(env, gamma, threshold):\n",
    "    n_state = env.observation_space.n\n",
    "    n_action = env.action_space.n\n",
    "    v = torch.zeros(n_state)\n",
    "    \n",
    "    while True: \n",
    "        v_temp = torch.empty(n_state)\n",
    "        for state in range(n_state):\n",
    "            v_action = torch.zeros(n_action)\n",
    "            for action in range(n_action):\n",
    "                for trans_prob, next_state, reward, _ in env.env.P[state][action]:\n",
    "                    v_action[action] += trans_prob*(reward + gamma * v[next_state])\n",
    "                v_temp[state] = torch.max(v_action)\n",
    "        max_delta = torch.max(torch.abs(v - v_temp))\n",
    "        v = v_temp.clone()\n",
    "        if max_delta <= threshold:\n",
    "            break\n",
    "            \n",
    "    return v \n",
    "\n",
    "v_optimal = value_iteration(env,gamma, threshold)\n",
    "print(\"Optimal value function: \", v_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488985bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m         optimal_policy[state] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(v_action)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m optimal_policy\n\u001b[1;32m---> 14\u001b[0m optimal_policy \u001b[38;5;241m=\u001b[39m extract_policy(\u001b[43menv\u001b[49m, v, gamma)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "def extract_policy(env, v, gamma):\n",
    "    n_state = env.observation_space.n\n",
    "    n_action = env.action_space.n\n",
    "    optimal_policy = torch.zeros(n_state, dtype=torch.int64)\n",
    "    \n",
    "    for state in range(n_state):\n",
    "        v_action = torch.zeros(n_action)\n",
    "        for action in range(n_action):\n",
    "            for trans_prob, new_state, reward, _ in env.env.P[state][action]:\n",
    "                v_action[action] += trans_prob*(reward + gamma * v_optimal[new_state])\n",
    "        optimal_policy[state] = torch.argmax(v_action)\n",
    "    return optimal_policy\n",
    "\n",
    "optimal_policy = extract_policy(env, v, gamma)\n",
    "print(\"Optimal policy: \\n\", optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf4d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#policy integration algorithm\n",
    "def policy_evaluation(env, policy, gamma, threshold):\n",
    "    n_state = policy.shape[0]\n",
    "    v = torch.zeros(n_state)\n",
    "    while True: \n",
    "        v_temp = torch.zeros(n_state)\n",
    "        for state in range(n_state):\n",
    "            action = policy[state].item()\n",
    "            for trans_prob, new_state, reward, _ in env.env.P[state][action]:\n",
    "                v_temp[state] += trans_prob*(reward + gamma * v[new_state])\n",
    "            max_delta = torch.max(torch.abs(v - v_temp))\n",
    "            v = v_temp.clone()\n",
    "            if max_delta <= threshold: \n",
    "                break\n",
    "    return v \n",
    "\n",
    "def policy_improvement(env, v,  gamma):\n",
    "    n_state = env.observation_space.n \n",
    "    n_action = env.action_space.n \n",
    "    policy = torch.zeros(n_state)\n",
    "    for state in range(n_state):\n",
    "        v_action = torch.zeros(n_action)\n",
    "        for action in range(n_action):\n",
    "            for trans_phob, new_state, reward, _ in env.env.P[state][action]:\n",
    "                v_action[action] += trans_phob *(reward + gamma*v[new_state])\n",
    "        policy[state] = torch.argmax(v_action)\n",
    "    return policy\n",
    "\n",
    "def policy_iteration(env, gamma, threshold):\n",
    "    n_state = env.observation_space.n\n",
    "    n_action = env.action_space.n\n",
    "    policy = torch.randint(high = n_action, size = (n_state,)).float()\n",
    "    while True:\n",
    "        v = policy_evaluation(env, policy, gamma, threshold)\n",
    "        new_policy = policy_improvement(env, v, gamma)\n",
    "        if torch.equal(policy, new_policy):\n",
    "            break\n",
    "        policy = new_policy.clone()\n",
    "    return policy    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e22207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
